# -*- coding: utf-8 -*-
"""Image_Captioning_New(2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18mNjKZBK6ufouHMqtMrUYPI7f1AY6DW9
"""

#install kaggle
!pip install kaggle
# Upload file
from google.colab import files
files.upload()
# Make dir for kaggle
! mkdir -p ~/.kaggle
! cp kaggle.json ~/.kaggle/
! chmod 600 ~/.kaggle/kaggle.json
# Kaggel API for dataset
! kaggle datasets download -d adityajn105/flickr8k

# Unzipping
! unzip flickr8k.zip

"""Checking the size of images"""

# Finding the shape of image
import cv2
import os
for i, image in enumerate(os.listdir('/content/Images')):
  img_path = os.path.join('/content/Images', image)
  img = cv2.imread(img_path)
  print(f"Image {i} Shape : {img.shape}")
  if i == 10:
    break

"""Importing the model"""

from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from tensorflow.keras.models import Model

model = VGG16()
model = Model(inputs=model.inputs, outputs=model.layers[-2].output)

print (model.summary())

from tqdm import tqdm
features = {}

for image in tqdm(os.listdir('/content/Images')):
  if image.endswith('.jpg') or image.endswith('.png'):
    img_path = os.path.join('/content/Images', image)
    img = load_img(img_path, target_size=(224, 224))
    img_array = img_to_array(img)
    img_array = preprocess_input(img_array)
    image_id = image.split('.')[0]
    features[image_id] = model.predict(img_array.reshape(1, 224, 224, 3), verbose=0)

features['1000268201_693b08cb0e'].shape

import pickle
with open('features.pkl', 'wb') as f:
  pickle.dump(features, f)

import pickle
with open('features.pkl', 'rb') as f:
  features = pickle.load(f)

with open('/content/captions.txt', 'r') as file:
    captions = file.read()

captions

"""Making a dictonary with key as image name and captions as value

"""

from tqdm import tqdm
mapping = {}

for text in tqdm(captions.split('\n')):
  tokens = text.split(',')
  if len(tokens) < 2:
    continue
  image_id, caption = tokens[0], tokens[1:]
  image_id = image_id.split('.')[0]
  caption = ' '.join(caption)
  if image_id not in mapping:
    mapping[image_id] = []
  mapping[image_id].append(caption)

print("\n Total size of mappings : ", len(mapping))

# First text in document is title of .txt file which need to be removed.
del mapping['image']

print("\n Total size of mappings : ", len(mapping))

"""Removeing all special charactors and extra spaces from captions"""

def clean(mapping):
    for key, captions in mapping.items():
        for i in range(len(captions)):
            caption = captions[i]
            caption = caption.lower()
            caption = caption.replace('[^A-Za-z]', '')
            caption = caption.replace('\s+', ' ')
            caption = 'startseq ' + " ".join([word for word in caption.split() if len(word)>1]) + ' endseq'
            captions[i] = caption

clean(mapping)

# Making all_captions for using it in tokenizing the words
all_captions = []

for key in mapping:
  for caption in mapping[key]:
      all_captions.append(caption)

print(all_captions)
print('Length of captions : ', len(all_captions))

"""Tokenizing the sentenses"""

from tensorflow.keras.preprocessing.text import Tokenizer

tokenizer = Tokenizer()
tokenizer.fit_on_texts(all_captions)
vocab_size = len(tokenizer.word_index) + 1
print("Total Unique words/Vocab size: ", vocab_size)
max_len_cap = max(len(caption.split()) for caption in all_captions)
print('Max lenght of caption: ', max_len_cap)

tokenizer.word_index

from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
import numpy as np
# Making data generator function

def data_generator(image_id, captions_mapping, image_features, tokenizer, batch_size, max_length, vocab_size):
  image_fet, seq_input_text, y_output = [], [], []
  n = 0
  while True:
    for key, captions in captions_mapping.items():
      n += 1

      for capt in captions:
        seq = tokenizer.texts_to_sequences([capt])[0]

        for i in range(1, len(seq)):
          input_seq, output_seq = seq[:i], seq[i]
          input_seq = pad_sequences([input_seq], maxlen=max_length)[0]
          output_seq = to_categorical([output_seq], num_classes=vocab_size)[0]

          image_fet.append(image_features[key][0])
          seq_input_text.append(input_seq)
          y_output.append(output_seq)

      if n == batch_size:
        image_fet, seq_input_text, y_output = np.array(image_fet), np.array(seq_input_text), np.array(y_output)
        yield [image_fet, seq_input_text], y_output
        image_fet, seq_input_text, y_output = [], [], []
        n = 0

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Dropout, Embedding, LSTM, concatenate, Add
from tensorflow.keras.utils import plot_model

# Image model layers
input1 = Input(shape=(4096,), name="image_input")
fe1 = Dropout(0.4, name="image_dropout")(input1)
fe2 = Dense(256, activation='relu', name="image_dense")(fe1)

# Text model layers
input2 = Input(shape=(max_len_cap,), name="text_input")
se1 = Embedding(vocab_size, 256, mask_zero=True, name="text_embedding")(input2)
se2 = Dropout(0.4, name="text_dropout")(se1)
se3 = LSTM(256, return_sequences=False, name="text_lstm")(se2)

# decoder model
decoder1 = Add()([fe2, se3])
decoder2 = Dense(256, activation='relu', name="decoder_dense")(decoder1)
outputs = Dense(vocab_size, activation='softmax', name="output_dense")(decoder2)

# defining the model
model = Model(inputs=[input1, input2], outputs=outputs)
model.compile(loss='categorical_crossentropy', optimizer='adam')

# plot the model
plot_model(model, show_shapes=True, show_layer_names=True)

from tensorflow.keras.callbacks import ModelCheckpoint

# Define the callback
checkpoint = ModelCheckpoint('model_epoch_{epoch:02d}.h5', save_weights_only=False, save_best_only=True, verbose=1)

# Training of Model
epochs = 10
batch_size = 32
steps = len(mapping.keys()) // batch_size

for epoch in range(epochs):
    # Create data generator
    generator = data_generator(
        image_id=list(mapping.keys()),
        captions_mapping=mapping,
        image_features=features,
        tokenizer=tokenizer,
        max_length=max_len_cap,
        vocab_size=vocab_size,
        batch_size=batch_size
    )

    # Fit for one epoch
    model.fit(generator, epochs=1, steps_per_epoch=steps, verbose=1, callbacks=[checkpoint])

    # Save the model manually (if needed)
    model.save(f'model_epoch_{epoch + 1}.h5')

"""Function to return word for highest prbable index predictions"""

def index_to_word(integer, tokenizer):
    for word, index in tokenizer.word_index.items():
        if index == integer:
            return word
    return None

# generate caption for an image
def predict_caption(model, image, tokenizer, max_length):
    # add start tag for generation process
    in_text = 'startseq'
    # iterate over the max length of sequence
    for i in range(max_length):
        # encode input sequence
        sequence = tokenizer.texts_to_sequences([in_text])[0]
        # pad the sequence
        sequence = pad_sequences([sequence], max_length)
        # predict next word
        yhat = model.predict([image, sequence], verbose=0)
        # get index with high probability
        yhat = np.argmax(yhat)
        # convert index to word
        word = index_to_word(yhat, tokenizer)
        # stop if word not found
        if word is None:
            break
        # append word as input for generating next word
        in_text += " " + word
        # stop if we reach end tag
        if word == 'endseq':
            break

    return in_text

from PIL import Image
import matplotlib.pyplot as plt
import os
def generate_caption(image_name):
    # load the image
    # image_name = "1001773457_577c3a7d70.jpg"
    image_id = image_name.split('.')[0]
    img_path = os.path.join('/content', "Images", image_name)
    image = Image.open(img_path)
    captions = mapping[image_id]
    print('---------------------Actual---------------------')
    for caption in captions:
        print(caption)
    # predict the caption
    y_pred = predict_caption(model, features[image_id], tokenizer, max_len_cap)
    print('--------------------Predicted--------------------')
    print(y_pred)
    plt.imshow(image)

generate_caption("1131800850_89c7ffd477.jpg")

# Convert tokenizer to JSON format
tokenizer_json = tokenizer.to_json()

# Save tokenizer JSON to file
with open('tokenizer.json', 'w', encoding='utf-8') as f:
    f.write(tokenizer_json)

# prompt: how can i save the weights of model

model.save_weights('model_weights.h5')